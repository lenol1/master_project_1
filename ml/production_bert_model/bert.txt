import joblib
import os
import csv
import json
from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional
from fastapi.middleware.cors import CORSMiddleware
from fastapi import BackgroundTasks
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
import joblib as _joblib
import config # –í–∞—à —Ñ–∞–π–ª config.py
import time
from datetime import datetime

# --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è FastAPI ---
app = FastAPI(
    title="ML Categorization Service",
    description="API –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è.",
    version="1.0.0"
)

# --- –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ —Ç–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∏ ---
global_model = None
global_predict_function = None

personalized_models_cache = {}

CORRECTIONS_FILE = "user_corrections.csv"
MODEL_STATUS_FILE = "user_model_status.json"
DYNAMIC_MAPPINGS_FILE = "dynamic_category_mappings.json"
user_model_status = {}
user_corrections_map = {}

# helper mapping of known id -> name (–±—É–¥–µ –æ–Ω–æ–≤–ª–µ–Ω–æ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º–∏ –º–∞–ø—ñ–Ω–≥–∞–º–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ)
ID_TO_NAME = {
    0: '–Ü–Ω—à–µ', 1: '–ü—Ä–æ–¥—É–∫—Ç–∏', 2: '–ö–∞—Ñ–µ', 3: '–û–Ω–ª–∞–π–Ω –ø–æ–∫—É–ø–∫–∏', 4: '–ï–ª–µ–∫—Ç—Ä–æ–Ω—ñ–∫–∞',
    5: '–ö–∞–Ω—Ü—Ç–æ–≤–∞—Ä–∏', 6: '–°—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç', 7: '–û–¥—è–≥', 8: '–ü–ª–∞—Ç–µ–∂—ñ/–¢–µ—Ä–º—ñ–Ω–∞–ª–∏',
    9: '–ü–µ—Ä–µ–∫–∞–∑', 10: '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç', 11: '–ú–æ–±—ñ–ª—å–Ω–∏–π', 12: '–ó–æ–æ—Ç–æ–≤–∞—Ä–∏', 13: "–ö–æ—Å–º–µ—Ç–∏–∫–∞",
    14: '–ü–æ–¥–∞—Ç–∫–∏/–ü–ª–∞—Ç–µ–∂—ñ –¥–µ—Ä–∂–∞–≤—ñ', 15: '–ö–æ–Ω–¥–∏—Ç–µ—Ä—Å—å–∫—ñ', 16: '–†—ñ–∑–Ω–µ'
}

# inverse mapping
NAME_TO_ID = {v: k for k, v in ID_TO_NAME.items()}

# --- HELPER FUNCTIONS ---

def get_or_create_category_id(category_name: str) -> int:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —ñ—Å–Ω—É—é—á–∏–π ID –∞–±–æ —Å—Ç–≤–æ—Ä—é—î –Ω–æ–≤–∏–π, —è–∫—â–æ –Ω–∞–∑–≤–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, 
    —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î –Ω–æ–≤—ñ –º–∞–ø—ñ–Ω–≥–∏ –Ω–∞ –¥–∏—Å–∫.
    """
    global ID_TO_NAME, NAME_TO_ID
    
    name = str(category_name).strip()
    if not name:
        return NAME_TO_ID.get('–Ü–Ω—à–µ', 0)
        
    if name in NAME_TO_ID:
        return NAME_TO_ID[name]

    try:
        max_id = max(ID_TO_NAME.keys()) if ID_TO_NAME else 0
        new_id = max_id + 1
        
        ID_TO_NAME[new_id] = name
        NAME_TO_ID[name] = new_id
        
        # –õ–û–ì–Ü–ö–ê –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –ù–ê –î–ò–°–ö
        try:
            dynamic_mappings = {str(k): v for k, v in ID_TO_NAME.items() if k > 16}
            with open(DYNAMIC_MAPPINGS_FILE, 'w', encoding='utf-8') as f:
                json.dump(dynamic_mappings, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö –º–∞–ø—ñ–Ω–≥—ñ–≤: {e}")
        
        print(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ –Ω–æ–≤–∏–π ID –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó: {name} -> {new_id}")
        return new_id
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –Ω–æ–≤–æ–≥–æ ID –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó {name}: {e}")
        return NAME_TO_ID.get('–Ü–Ω—à–µ', 0)

def map_category_name_to_id(name_val: str):
    if not name_val:
        return None
    nv = str(name_val).strip()
    if nv in NAME_TO_ID:
        return int(NAME_TO_ID[nv])
    low = nv.lower()
    for k, nm in ID_TO_NAME.items():
        if low in nm.lower() or nm.lower() in low:
            return int(k)
    if '–ª—ñ–∫' in low or '–∞–ø—Ç–µ' in low:
        return 13
    return None

def map_category_id_to_name(id_val: int):
    """–ü–æ–≤–µ—Ä—Ç–∞—î —ñ–º'—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∑–∞ —á–∏—Å–ª–æ–≤–∏–º ID. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≥–ª–æ–±–∞–ª—å–Ω–∏–π ID_TO_NAME."""
    return ID_TO_NAME.get(id_val, '–ù–µ–≤—ñ–¥–æ–º–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è')

def normalize_description(desc: str):
    if desc is None:
        return ''
    return str(desc).strip().lower()

def load_model_status():
    global user_model_status
    try:
        if os.path.exists(MODEL_STATUS_FILE):
            with open(MODEL_STATUS_FILE, 'r', encoding='utf-8') as f:
                import json
                user_model_status = json.load(f)
                print('Loaded model status file for users:', list(user_model_status.keys()))
    except Exception as e:
        print('Could not load model status file:', e)


def load_user_corrections():
    """Load corrections CSV into user_corrections_map (in-memory), to allow exact-match overrides."""
    global user_corrections_map
    user_corrections_map = {}
    if not os.path.exists(CORRECTIONS_FILE):
        return
    try:
        try:
            corr_df = pd.read_csv(CORRECTIONS_FILE, encoding='utf-8-sig', engine='python', on_bad_lines='skip')
        except TypeError:
            corr_df = pd.read_csv(CORRECTIONS_FILE, encoding='utf-8-sig', engine='python')

        if corr_df is None or corr_df.empty:
            import csv as _csv
            with open(CORRECTIONS_FILE, 'r', encoding='utf-8-sig', newline='') as f:
                reader = _csv.reader(f)
                rows = list(reader)
                if not rows: return
                data_rows = rows[1:]
                normalized = []
                for i, cells in enumerate(data_rows, start=2):
                    if len(cells) < 2: continue
                    user_id = cells[0].strip() if len(cells) > 0 else ''
                    description = cells[1].strip() if len(cells) > 1 else ''
                    corrected_category_id = cells[3].strip() if len(cells) > 3 else ''
                    corrected_category_name = ','.join([c.strip() for c in cells[5:]]) if len(cells) > 5 else (cells[5].strip() if len(cells) == 6 else '')
                    normalized.append({
                        'user_id': user_id, 'description': description, 'corrected_category_id': corrected_category_id,
                        'corrected_category_name': corrected_category_name
                    })
                corr_df = pd.DataFrame(normalized)

        for _, row in corr_df.iterrows():
            uid = str(row.get('user_id', '')).strip()
            desc = normalize_description(row.get('description', ''))
            cat_id = None
            raw_id = row.get('corrected_category_id', None)
            if raw_id is not None and not pd.isna(raw_id) and str(raw_id).strip() != '':
                try: cat_id = int(float(raw_id))
                except Exception: cat_id = None

            if cat_id is None:
                name_val = row.get('corrected_category_name', '')
                cat_id = map_category_name_to_id(name_val) 

            if uid and desc and cat_id is not None:
                user_corrections_map.setdefault(uid, {})[desc] = int(cat_id)
    except Exception as e:
        print('‚ö†Ô∏è Failed to load corrections into memory:', e)

def save_model_status():
    try:
        import json
        with open(MODEL_STATUS_FILE, 'w', encoding='utf-8') as f:
            json.dump(user_model_status, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print('Could not save model status file:', e)


# --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ì–ª–æ–±–∞–ª—å–Ω–æ—ó –ú–æ–¥–µ–ª—ñ (–ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ —Å–µ—Ä–≤–µ—Ä–∞) ---
@app.on_event("startup")
def load_global_model():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –æ–¥–Ω—É "—á–µ–º–ø—ñ–æ–Ω—Å—å–∫—É" –º–æ–¥–µ–ª—å (BERT –∞–±–æ RF) —Ç–∞ –¥–∏–Ω–∞–º—ñ—á–Ω—ñ –º–∞–ø—ñ–Ω–≥–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π.
    """
    global global_model, global_predict_function
    global ID_TO_NAME, NAME_TO_ID
    
    try:
        if config.MODEL_TYPE == "BERT":
            # --- –õ–û–ì–Ü–ö–ê –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø BERT ---
            try:
                import torch
                from transformers import BertTokenizerFast, BertForSequenceClassification, TextClassificationPipeline
            except Exception as e:
                print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ transformers/torch: {e}")
                raise

            local_model_path = config.BERT_MODEL_PATH
            
            print(f"üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è BERT: –ú–æ–¥–µ–ª—å —Ç–∞ –¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä –∑ '{local_model_path}'...")
            
            try:
                tokenizer = BertTokenizerFast.from_pretrained(
                    local_model_path, 
                    local_files_only=False
                ) 
                model = BertForSequenceClassification.from_pretrained(
                    local_model_path, 
                    local_files_only=True,
                    use_safetensors=True
                )
                
                device = 0 if torch.cuda.is_available() else -1
                global_model = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device)
                
                def bert_predict(text: str) -> int:
                    # –ü—Ä–æ–≥–Ω–æ–∑ BERT
                    results = global_model(text, top_k=1)
                    if not results: return 0
                        
                    result = results[0]
                    label_str = result['label']
                    
                    if "LABEL_" in label_str:
                        predicted_id = int(label_str.split('_')[-1])
                    else:
                        try:
                            predicted_id = int(label_str)
                        except ValueError:
                            # –Ø–∫—â–æ –º–æ–¥–µ–ª—å –ø–æ–≤–µ—Ä—Ç–∞—î –Ω–∞–∑–≤—É, —Å–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ ID –∑–∞ –Ω–∞–∑–≤–æ—é
                            predicted_id = map_category_name_to_id(label_str) or 0
                            
                    print(f"DEBUG: BERT Output: '{label_str}', Mapped ID: {predicted_id}, Category: {map_category_id_to_name(predicted_id)}")

                    return predicted_id

                global_predict_function = bert_predict
                print(f"‚úÖ –£—Å–ø—ñ—Ö! –ì–ª–æ–±–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å BERT –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞.")
                
            except Exception as e:
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è BERT: {e}")
                print(f"–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ: 1. –ß–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —à–ª—è—Ö '{local_model_path}'? 2. –ß–∏ —î —É –ø–∞–ø—Ü—ñ config.json —Ç–∞ model.safetensors?")
                raise e
            # --- –ö–Ü–ù–ï–¶–¨ –õ–û–ì–Ü–ö–ò –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø BERT ---

        elif config.MODEL_TYPE == "RF":
            # --- –õ–û–ì–Ü–ö–ê –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø RF ---
            model_path = config.SKLEARN_MODEL_PATH
            print(f"üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ì–õ–û–ë–ê–õ–¨–ù–û–á –º–æ–¥–µ–ª—ñ SKlearn –∑ {model_path}...")

            if not os.path.exists(model_path):
                print(f"‚ö†Ô∏è SKlearn model file not found at {model_path}. Please train the model (run train.py) or place the model file there.")
            else:
                global_model = joblib.load(model_path)
                
                def rf_predict(text: str) -> int:
                    result = global_model.predict([text])[0]
                    return int(result)

                global_predict_function = rf_predict
                print(f"‚úÖ –ì–ª–æ–±–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å ({config.MODEL_TYPE}) —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")
            # --- –ö–Ü–ù–ï–¶–¨ –õ–û–ì–Ü–ö–ò –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø RF ---
            
        else:
            print(f"‚ùå‚ùå‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê: –ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π —Ç–∏–ø –º–æ–¥–µ–ª—ñ: {config.MODEL_TYPE}")
            
    except Exception as e:
        print(f"‚ùå‚ùå‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê: –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≥–ª–æ–±–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å. {e}")

    # --- –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ò–ù–ê–ú–Ü–ß–ù–ò–• –ú–ï–ü–ü–Ü–ù–ì–Ü–í (–ü–ï–†–°–ò–°–¢–ï–ù–¢–ù–Ü–°–¢–¨ –ö–ê–¢–ï–ì–û–†–Ü–ô) ---
    try:
        if os.path.exists(DYNAMIC_MAPPINGS_FILE):
            import json
            with open(DYNAMIC_MAPPINGS_FILE, 'r', encoding='utf-8') as f:
                dynamic_mappings = json.load(f)
                
            updated_count = 0
            for k_str, v in dynamic_mappings.items():
                k = int(k_str)
                if k not in ID_TO_NAME:
                    ID_TO_NAME[k] = v
                    NAME_TO_ID[v] = k
                    updated_count += 1
            if updated_count > 0:
                 print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {updated_count} –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö –º–∞–ø—ñ–Ω–≥—ñ–≤ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑ {DYNAMIC_MAPPINGS_FILE}. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π ID: {max(ID_TO_NAME.keys())}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö –º–∞–ø—ñ–Ω–≥—ñ–≤: {e}")
    # --- –ö–Ü–ù–ï–¶–¨ –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ò–ù–ê–ú–Ü–ß–ù–ò–• –ú–ï–ü–ü–Ü–ù–ì–Ü–í ---

    load_model_status()
    load_user_corrections()


# --- 3. –û–ø–∏—Å –ú–æ–¥–µ–ª–µ–π –î–∞–Ω–∏—Ö (Pydantic) ---

class TransactionInput(BaseModel):
    description: str
    user_id: str

class CorrectionInput(BaseModel):
    user_id: str
    description: str
    original_category_id: Optional[int] = None 
    corrected_category_id: Optional[int] = None 
    original_category_name: Optional[str] = None
    corrected_category_name: Optional[str] = None

class CategorizationOutput(BaseModel):
    description: str
    category_id: int
    category_name: str


# --- 4. –õ–æ–≥—ñ–∫–∞ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –í–∏–ø—Ä–∞–≤–ª–µ–Ω—å ---

def save_correction_to_csv(correction: CorrectionInput):
    """
    –î–æ–ø–∏—Å—É—î –Ω–æ–≤–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤ CSV-—Ñ–∞–π–ª.
    –ü—Ä–∏–∑–Ω–∞—á–∞—î —á–∏—Å–ª–æ–≤–∏–π ID, —è–∫—â–æ –Ω–∞–¥–∞–Ω–æ –ª–∏—à–µ –Ω–∞–∑–≤—É –Ω–æ–≤–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó (–∑–±–µ—Ä—ñ–≥–∞—é—á–∏ ID –Ω–∞ –¥–∏—Å–∫).
    """
    file_exists = os.path.isfile(CORRECTIONS_FILE)
    
    # --- –õ–û–ì–Ü–ö–ê –î–õ–Ø –ü–†–ò–ó–ù–ê–ß–ï–ù–ù–Ø ID ---
    corrected_id = correction.corrected_category_id
    corrected_name = correction.corrected_category_name
    
    if corrected_id is None and corrected_name:
        mapped_id = map_category_name_to_id(corrected_name)
        if mapped_id is None:
            corrected_id = get_or_create_category_id(corrected_name)
        else:
            corrected_id = mapped_id
            
    correction.corrected_category_id = corrected_id
    # --- –ö–Ü–ù–ï–¶–¨ –õ–û–ì–Ü–ö–ò ---
    
    with open(CORRECTIONS_FILE, 'a', newline='', encoding='utf-8') as f:
        fieldnames = ['user_id', 'description', 'original_category_id', 'corrected_category_id', 'original_category_name', 'corrected_category_name']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        
        row = {
            'user_id': correction.user_id, 'description': correction.description, 
            'original_category_id': correction.original_category_id,
            'corrected_category_id': corrected_id, 
            'original_category_name': correction.original_category_name,
            'corrected_category_name': corrected_name
        }
        writer.writerow(row)
        
    try:
        uid = str(correction.user_id).strip()
        desc = normalize_description(correction.description)
        cat_id = corrected_id 
        
        if uid and desc and cat_id is not None:
            user_corrections_map.setdefault(uid, {})[desc] = int(cat_id)
            print(f"[Corrections map] Updated in-memory corrections for user {uid}: '{desc}' -> {cat_id}")
    except Exception as e:
        print('‚ö†Ô∏è Failed to update in-memory corrections map:', e)

def retrain_personalized_model(user_id: str):
    """
    –§–æ–Ω–æ–≤–µ –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ—ó –º–æ–¥–µ–ª—ñ (Random Forest) –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.
    """
    try:
        print(f"üîß Retraining personalized model for user {user_id}...")
        
        if not os.path.exists(config.DATA_FILE):
            print('‚ö†Ô∏è Global data file not found, skipping personalized training')
            return

        df = pd.read_csv(config.DATA_FILE, encoding='utf-8-sig', sep=',', engine='python', on_bad_lines='skip')
        
        # –ß–∏—Ç–∞—î–º–æ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        user_corrections = []
        if os.path.exists(CORRECTIONS_FILE):
            try:
                try:
                    corr_df = pd.read_csv(CORRECTIONS_FILE, encoding='utf-8-sig', engine='python', on_bad_lines='skip')
                except TypeError:
                    corr_df = pd.read_csv(CORRECTIONS_FILE, encoding='utf-8-sig', engine='python')

                if corr_df is None or corr_df.empty:
                    import csv as _csv
                    with open(CORRECTIONS_FILE, 'r', encoding='utf-8-sig', newline='') as f:
                        reader = _csv.reader(f)
                        rows = list(reader)
                        if not rows: corr_rows = []
                        else:
                            header = rows[0]
                            data_rows = rows[1:]
                            normalized = []
                            for i, cells in enumerate(data_rows, start=2):
                                if len(cells) < 2: continue
                                user = cells[0].strip() if len(cells) > 0 else ''
                                desc = cells[1].strip() if len(cells) > 1 else ''
                                orig_id = cells[2].strip() if len(cells) > 2 else ''
                                corr_id = cells[3].strip() if len(cells) > 3 else ''
                                orig_name = cells[4].strip() if len(cells) > 4 else ''
                                corr_name = ','.join([c.strip() for c in cells[5:]]) if len(cells) > 5 else (cells[5].strip() if len(cells) == 6 else '')
                                normalized.append({
                                    'user_id': user, 'description': desc, 'original_category_id': orig_id,
                                    'corrected_category_id': corr_id, 'original_category_name': orig_name,
                                    'corrected_category_name': corr_name
                                })
                            corr_df = pd.DataFrame(normalized)
                            
                            try:
                                expected_cols = ['user_id', 'description', 'original_category_id', 'corrected_category_id', 'original_category_name', 'corrected_category_name']
                                if len(header) < len(expected_cols) or any(c not in header for c in expected_cols):
                                    import shutil
                                    from datetime import datetime 
                                    bak_name = f"{CORRECTIONS_FILE}.bak.{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
                                    try:
                                        shutil.copy(CORRECTIONS_FILE, bak_name)
                                        print(f"‚ÑπÔ∏è Corrections file header inconsistent ‚Äî created backup: {bak_name}")
                                        corr_df.to_csv(CORRECTIONS_FILE, index=False, encoding='utf-8-sig')
                                        print(f"‚úÖ Wrote cleaned corrections file with normalized 6-column header: {CORRECTIONS_FILE}")
                                    except Exception as wr_err:
                                        print('‚ö†Ô∏è Failed to backup/overwrite corrections file:', wr_err)
                            except Exception:
                                pass

                if corr_df is not None and not corr_df.empty:
                    corr_rows = corr_df[corr_df['user_id'] == user_id]
                else:
                    corr_rows = pd.DataFrame(columns=['user_id','description','original_category_id','corrected_category_id','original_category_name','corrected_category_name'])
                
                def resolve_category_id(row):
                    val = row.get('corrected_category_id', None)
                    if val is not None and not (pd.isna(val)):
                        try: return int(float(val))
                        except Exception: pass
                    name_val = row.get('corrected_category_name', '')
                    if isinstance(name_val, float) and pd.isna(name_val): name_val = ''
                    name_val = str(name_val).strip()
                    if name_val:
                        mapped = map_category_name_to_id(name_val) 
                        if mapped is not None: return mapped
                    return None

                for _, row in corr_rows.iterrows():
                    desc = str(row.get('description', '')).strip()
                    corrected = resolve_category_id(row)
                    if desc and corrected is not None:
                        user_corrections.append({ 'text_features': desc, 'category_id': int(corrected) })
                        try:
                            user_corrections_map.setdefault(user_id, {})[normalize_description(desc)] = int(corrected)
                        except Exception: pass
            except Exception as rc_err:
                print('‚ö†Ô∏è Failed to read corrections file:', rc_err)

        print(f"‚ÑπÔ∏è Found {len(user_corrections)} correction rows for user {user_id} to include in retraining")
        if len(user_corrections) == 0:
            print(f'‚ÑπÔ∏è No corrections found for {user_id}, skipping personalized training')
            return

        # –ù–∞–≤—á–∞—î–º–æ Random Forest (RF) –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—ó
        df_train = df[['text_features', 'category_id']].copy()
        corr_df_user = pd.DataFrame(user_corrections)
        df_comb = pd.concat([df_train, corr_df_user], ignore_index=True)

        X = df_comb['text_features']
        y = df_comb['category_id']

        pipeline = Pipeline([
            ('tfidf', TfidfVectorizer()),
            ('rf', RandomForestClassifier(random_state=42, n_jobs=-1))
        ])

        pipeline.fit(X, y)

        target_path = f"model_user_{user_id}.joblib"
        _joblib.dump(pipeline, target_path)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ –∫–µ—à —Ç–∞ —Å—Ç–∞—Ç—É—Å
        def user_predict(text: str) -> int:
            return int(pipeline.predict([text])[0])

        personalized_models_cache[user_id] = user_predict
        
        from datetime import datetime 
        user_model_status[user_id] = datetime.utcnow().isoformat()
        save_model_status()
        print(f"‚úÖ Personalized model for {user_id} trained and saved to {target_path}")
        
    except Exception as e:
        print('‚ùå Error retraining personalized model:', e)


# --- 5. –ö—ñ–Ω—Ü–µ–≤—ñ –¢–æ—á–∫–∏ (Endpoints) API ---

@app.post("/api/v1/categorize", response_model=CategorizationOutput)
def categorize_transaction(transaction: TransactionInput):
    """
    –ü—Ä–∏–π–º–∞—î —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—é —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –∫–∞—Ç–µ–≥–æ—Ä—ñ—é —Ç–∞ —ó—ó ID/–ù–∞–∑–≤—É.
    """
    global global_predict_function, personalized_models_cache
    
    predict_function_to_use = None
    user_id = transaction.user_id
    
    personalized_model_path = f"model_user_{user_id}.joblib" 
    start_time = time.time()
    
    # 1. Check exact-match user corrections (fastest override)
    try:
        desc_norm = normalize_description(transaction.description)
        if user_id and desc_norm and user_corrections_map.get(user_id) and desc_norm in user_corrections_map[user_id]:
            corrected_id = int(user_corrections_map[user_id][desc_norm])
            corrected_name = map_category_id_to_name(corrected_id)
            print(f"[MODEL SELECT] –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ —Ç–æ—á–Ω–µ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è (Override) –¥–ª—è {user_id}")
            return { 
                'description': transaction.description, 'category_id': corrected_id, 'category_name': corrected_name
            }
    except Exception as e:
        print('‚ö†Ô∏è Error checking user corrections map:', e)

    # 2. –í–∏–±—ñ—Ä –º–æ–¥–µ–ª—ñ: –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞ (RF)
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ RF –º–æ–¥–µ–ª—å –≤–∂–µ —î –≤ –∫–µ—à—ñ
    if user_id in personalized_models_cache:
        predict_function_to_use = personalized_models_cache[user_id]
        
        category_id = predict_function_to_use(transaction.description)
        category_name = map_category_id_to_name(category_id)
        duration_ms = (time.time() - start_time) * 1000
        print(f"[MODEL SELECT] –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å RF (Cache HIT) –¥–ª—è {user_id}. –ö–∞—Ç–µ–≥–æ—Ä—ñ—è: {category_name}")
        return {
             "description": transaction.description, "category_id": category_id, "category_name": category_name
        }
        
    # –Ø–∫—â–æ –Ω–µ–º–∞—î –≤ –∫–µ—à—ñ, –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞ –¥–∏—Å–∫—É
    elif os.path.exists(personalized_model_path):
        print(f"[MODEL SELECT] –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å RF –∑–Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ –¥–∏—Å–∫—É –¥–ª—è {user_id}")
        try:
            personalized_model = joblib.load(personalized_model_path)
            
            # --- –õ–û–ì–Ü–ö–ê –î–õ–Ø RF-–ü–†–û–ì–ù–û–ó–£ –ó –õ–û–ì–£–í–ê–ù–ù–Ø–ú –¢–ê –ü–û–í–ï–†–ù–ï–ù–ù–Ø–ú ---
            text_input = transaction.description
            rf_predicted_id = int(personalized_model.predict([text_input])[0]) # –í–ò–ö–û–ù–£–Ñ–ú–û –ü–†–û–ì–ù–û–ó
            
            # –ö–µ—à—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è –º–∞–π–±—É—Ç–Ω—ñ—Ö –∑–∞–ø–∏—Ç—ñ–≤
            def personalized_predict(text: str) -> int:
                return int(personalized_model.predict([text])[0])

            personalized_models_cache[user_id] = personalized_predict
            
            rf_predicted_name = map_category_id_to_name(rf_predicted_id)
            print(f"[MODEL SELECT] –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å RF —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ —Ç–∞ –ü–†–û–ì–ù–û–ó–£–Ñ: ID {rf_predicted_id} ({rf_predicted_name})")

            end_time = time.time()
            duration_ms = (end_time - start_time) * 1000

            print(f"‚è±Ô∏è [RF PERFORMANCE] –ß–∞—Å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É: {duration_ms:.2f} –º—Å")
            
            return { # <--- –Ø–í–ù–ò–ô RETURN –î–õ–Ø RF
                "description": transaction.description,
                "category_id": rf_predicted_id,
                "category_name": rf_predicted_name
            }
            
        except Exception as e:
            # –Ø–∫—â–æ RF –º–æ–¥–µ–ª—å –ø–æ—à–∫–æ–¥–∂–µ–Ω–∞, –ø–µ—Ä–µ—Ö–æ–¥–∏–º–æ –¥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ—ó (BERT)
            print(f"‚ùå [MODEL SELECT ERROR] –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ: {e}. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ì–õ–û–ë–ê–õ–¨–ù–ê.")
            predict_function_to_use = global_predict_function 
            
    # 3. –í–∏–±—ñ—Ä –º–æ–¥–µ–ª—ñ: –ì–ª–æ–±–∞–ª—å–Ω–∞ (BERT)
    else:
        predict_function_to_use = global_predict_function
        print(f"[MODEL SELECT] –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –ì–õ–û–ë–ê–õ–¨–ù–£ –º–æ–¥–µ–ª—å ({config.MODEL_TYPE}).")
            
    if predict_function_to_use is None:
        return {"error": "–ì–ª–æ–±–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞"}, 500
        
    # –í–ò–ö–û–ù–ê–ù–ù–Ø –ì–õ–û–ë–ê–õ–¨–ù–û–ì–û –ü–†–û–ì–ù–û–ó–£ (BERT)
    try:
        category_id = predict_function_to_use(transaction.description)
        end_time = time.time()
        duration_ms = (end_time - start_time) * 1000
        
        category_name = map_category_id_to_name(category_id)
        
        print(f"‚è±Ô∏è [BERT PERFORMANCE] –ß–∞—Å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É: {duration_ms:.2f} –º—Å, –ö–∞—Ç–µ–≥–æ—Ä—ñ—è: {category_name}")
        return {
            "description": transaction.description,
            "category_id": category_id,
            "category_name": category_name
        }
    except Exception as e:
        print(f"‚ùå [PREDICTION ERROR] –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è: {str(e)}")
        # –ü—Ä–∏ –∫—Ä–∏—Ç–∏—á–Ω—ñ–π –ø–æ–º–∏–ª—Ü—ñ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ "–Ü–Ω—à–µ"
        return {
             "description": transaction.description,
             "category_id": 0,
             "category_name": map_category_id_to_name(0)
        }


@app.get('/api/v1/user-model-status/{user_id}')
def get_user_model_status(user_id: str):
    """Return simple status for user's personalized model: if exists and when it was last trained."""
    try:
        path = f"model_user_{user_id}.joblib"
        exists = os.path.exists(path)
        last_trained = user_model_status.get(user_id) if user_model_status else None
        return { 'user_id': user_id, 'model_exists': exists, 'last_trained': last_trained }
    except Exception as e:
        return { 'error': str(e) }, 500


@app.get('/api/v1/user-corrections/{user_id}')
def get_user_corrections(user_id: str):
    """Return in-memory corrections map for a user (description->category_id) for debugging/inspection."""
    try:
        data = user_corrections_map.get(user_id, {})
        return { 'user_id': user_id, 'corrections': data }
    except Exception as e:
        return { 'error': str(e) }, 500


@app.post("/api/v1/submit-correction")
def submit_correction(correction: CorrectionInput, background_tasks: BackgroundTasks):
    try:
        save_correction_to_csv(correction)
        background_tasks.add_task(retrain_personalized_model, correction.user_id)
        print(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤—ñ–¥ {correction.user_id}")
        return {"status": "correction_received"}
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è: {str(e)}")
        return {"error": f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è: {str(e)}"}, 500


# --- 6. –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è CORS ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:5000"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

if __name__ == "__main__":
    import uvicorn
    print("üöÄ –ó–∞–ø—É—Å–∫ ML API-—Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ http://localhost:8000")
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=True)