# ml/api.py
import joblib
import torch
import os
import csv
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline
import config  # –í–∞—à —Ñ–∞–π–ª config.py

# --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è FastAPI ---
app = FastAPI(
    title="ML Categorization Service",
    description="API –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è.",
    version="1.0.0"
)

# --- –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è –º–æ–¥–µ–ª–µ–π ---
# 'global_model' - —Ü–µ –Ω–∞–≤—á–µ–Ω–∞ –º–æ–¥–µ–ª—å (pipeline)
# 'global_predict_function' - —Ü–µ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è, —è–∫–∞ –ø—Ä–∏–π–º–∞—î —Ç–µ–∫—Å—Ç
global_model = None
global_predict_function = None

# –ö–µ—à –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏—Ö –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π (—â–æ–± –Ω–µ —á–∏—Ç–∞—Ç–∏ –∑ –¥–∏—Å–∫–∞ —â–æ—Ä–∞–∑—É)
personalized_models_cache = {}

# –§–∞–π–ª –¥–ª—è –∑–±–æ—Ä—É –≤–∏–ø—Ä–∞–≤–ª–µ–Ω—å –≤—ñ–¥ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
CORRECTIONS_FILE = "user_corrections.csv"


# --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ì–ª–æ–±–∞–ª—å–Ω–æ—ó –ú–æ–¥–µ–ª—ñ (–ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ —Å–µ—Ä–≤–µ—Ä–∞) ---
@app.on_event("startup")
def load_global_model():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –æ–¥–Ω—É "—á–µ–º–ø—ñ–æ–Ω—Å—å–∫—É" –º–æ–¥–µ–ª—å (RF –∞–±–æ BERT) –∑ config.py 
    –≤ –ø–∞–º'—è—Ç—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ —Å–µ—Ä–≤–µ—Ä–∞.
    """
    global global_model, global_predict_function
    
    try:
        if config.MODEL_TYPE == "BERT":
            model_path = config.BERT_MODEL_PATH
            print(f"üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ì–õ–û–ë–ê–õ–¨–ù–û–á –º–æ–¥–µ–ª—ñ BERT –∑ {model_path}...")
            
            tokenizer = BertTokenizer.from_pretrained(model_path)
            model = BertForSequenceClassification.from_pretrained(model_path)
            device = 0 if torch.cuda.is_available() else -1
            global_model = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device)
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è BERT
            def bert_predict(text: str) -> int:
                result = global_model(text)[0]
                return int(result['label'].split('_')[-1])
            
            global_predict_function = bert_predict

        elif config.MODEL_TYPE == "RF":
            model_path = config.SKLEARN_MODEL_PATH
            print(f"üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ì–õ–û–ë–ê–õ–¨–ù–û–á –º–æ–¥–µ–ª—ñ SKlearn –∑ {model_path}...")
            
            global_model = joblib.load(model_path)
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è SKlearn
            def rf_predict(text: str) -> int:
                # –ú–æ–¥–µ–ª—å (pipeline) –æ—á—ñ–∫—É—î —Å–ø–∏—Å–æ–∫
                result = global_model.predict([text])[0]
                return int(result)
            
            global_predict_function = rf_predict
        
        print(f"‚úÖ –ì–ª–æ–±–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å ({config.MODEL_TYPE}) —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

    except Exception as e:
        print(f"‚ùå‚ùå‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê: –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≥–ª–æ–±–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å. {e}")


# --- 3. –û–ø–∏—Å –ú–æ–¥–µ–ª–µ–π –î–∞–Ω–∏—Ö (Pydantic) ---

class TransactionInput(BaseModel):
    description: str
    user_id: str  # –í–∞–∂–ª–∏–≤–æ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—ó

class CorrectionInput(BaseModel):
    user_id: str
    description: str
    original_category_id: int  # –Ø–∫—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –∑–∞–ø—Ä–æ–ø–æ–Ω—É–≤–∞–ª–∞ –º–æ–¥–µ–ª—å
    corrected_category_id: int # –Ø–∫—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –æ–±—Ä–∞–≤ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á


# --- 4. –õ–æ–≥—ñ–∫–∞ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –í–∏–ø—Ä–∞–≤–ª–µ–Ω—å ---

def save_correction_to_csv(correction: CorrectionInput):
    """
    –î–æ–ø–∏—Å—É—î –Ω–æ–≤–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤ CSV-—Ñ–∞–π–ª (–Ω–∞—à—É "—Å–∫–∞—Ä–±–Ω–∏—á–∫—É").
    """
    file_exists = os.path.isfile(CORRECTIONS_FILE)
    
    with open(CORRECTIONS_FILE, 'a', newline='', encoding='utf-8') as f:
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Pydantic .model_dump() –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ª–æ–≤–Ω–∏–∫–∞
        writer = csv.DictWriter(f, fieldnames=correction.model_dump().keys())
        if not file_exists:
            writer.writeheader()  # –ù–∞–ø–∏—Å–∞—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏, —è–∫—â–æ —Ñ–∞–π–ª –Ω–æ–≤–∏–π
        writer.writerow(correction.model_dump())


# --- 5. –ö—ñ–Ω—Ü–µ–≤—ñ –¢–æ—á–∫–∏ (Endpoints) API ---

@app.post("/api/v1/categorize")
def categorize_transaction(transaction: TransactionInput):
    """
    –ì–û–õ–û–í–ù–ò–ô ENDPOINT: –ü—Ä–∏–π–º–∞—î —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—é, –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –ø–æ—Ç—Ä—ñ–±–Ω—É –º–æ–¥–µ–ª—å 
    (–ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—É –∞–±–æ –≥–ª–æ–±–∞–ª—å–Ω—É) —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –∫–∞—Ç–µ–≥–æ—Ä—ñ—é.
    """
    global global_predict_function, personalized_models_cache
    
    predict_function_to_use = None
    user_id = transaction.user_id
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ —à–ª—è—Ö –¥–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ (–ø—Ä–∏–ø—É—Å—Ç–∏–º–æ, –≤–æ–Ω–∏ –≤—Å—ñ SKlearn/joblib)
    personalized_model_path = f"model_user_{user_id}.joblib" 

    # --- –õ–û–ì–Ü–ö–ê –ê–î–ê–ü–¢–ê–¶–Ü–á (–í–ê–®–ê –ù–ê–£–ö–û–í–ê –ù–û–í–ò–ó–ù–ê) ---
    
    # 1. –ß–∏ —î —Ü—è –º–æ–¥–µ–ª—å –≤–∂–µ —É –∫–µ—à—ñ?
    if user_id in personalized_models_cache:
        predict_function_to_use = personalized_models_cache[user_id]
        print(f"[Cache HIT] –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –∫–µ—à—É –¥–ª—è {user_id}")

    # 2. –Ø–∫—â–æ –Ω—ñ, —á–∏ —ñ—Å–Ω—É—î –¥–ª—è —Ü—å–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π —Ñ–∞–π–ª?
    elif os.path.exists(personalized_model_path):
        print(f"[Cache MISS] –ó–Ω–∞–π–¥–µ–Ω–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å –Ω–∞ –¥–∏—Å–∫—É –¥–ª—è {user_id}")
        try:
            # (–î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏, –ø—Ä–∏–ø—É—Å—Ç–∏–º–æ, –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ - —Ü–µ SKlearn)
            personalized_model = joblib.load(personalized_model_path)
            
            def personalized_predict(text: str) -> int:
                return int(personalized_model.predict([text])[0])
            
            predict_function_to_use = personalized_predict
            personalized_models_cache[user_id] = predict_function_to_use # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –∫–µ—à
            
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ {personalized_model_path}: {e}")
            predict_function_to_use = global_predict_function # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≥–ª–æ–±–∞–ª—å–Ω—É
    
    # 3. –Ø–∫—â–æ –Ω—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≥–ª–æ–±–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å
    else:
        print(f"[Cache MISS] –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ì–õ–û–ë–ê–õ–¨–ù–û–á –º–æ–¥–µ–ª—ñ –¥–ª—è {user_id}")
        predict_function_to_use = global_predict_function
            
    # --- –í–∏–∫–æ–Ω–∞–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—É ---
    if predict_function_to_use is None:
        return {"error": "–ì–ª–æ–±–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞"}, 500
        
    try:
        category_id = predict_function_to_use(transaction.description)
        return {
            "description": transaction.description,
            "category_id": category_id
        }
    except Exception as e:
        return {"error": f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è: {str(e)}"}, 400


@app.post("/api/v1/submit-correction")
def submit_correction(correction: CorrectionInput):
    """
    ENDPOINT –ó–í–û–†–û–¢–ù–û–ì–û –ó–í'–Ø–ó–ö–£: –ü—Ä–∏–π–º–∞—î –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤—ñ–¥ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ 
    —ñ –∑–±–µ—Ä—ñ–≥–∞—î –π–æ–≥–æ –≤ "—Å–∫–∞—Ä–±–Ω–∏—á–∫—É" (CSV-—Ñ–∞–π–ª) –¥–ª—è –º–∞–π–±—É—Ç–Ω—å–æ–≥–æ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è.
    """
    try:
        save_correction_to_csv(correction)
        print(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤—ñ–¥ {correction.user_id}")
        return {"status": "correction_received"}
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è: {str(e)}")
        return {"error": f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è: {str(e)}"}, 500


# --- 6. –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è CORS ---
# –î–æ–∑–≤–æ–ª—è—î –≤–∞—à–æ–º—É React (–ø–æ—Ä—Ç 3000) —Ç–∞ Node.js (–ø–æ—Ä—Ç 5000)
# —Å–ø—ñ–ª–∫—É–≤–∞—Ç–∏—Å—è –∑ —Ü–∏–º Python-—Å–µ—Ä–≤–µ—Ä–æ–º (–ø–æ—Ä—Ç 8000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", # –í–∞—à React-–¥–æ–¥–∞—Ç–æ–∫
        "http://localhost:5000"  # –í–∞—à Node.js-—Å–µ—Ä–≤–µ—Ä
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- (–î–ª—è –∑–∞–ø—É—Å–∫—É uvicorn –∑ —Ç–µ—Ä–º—ñ–Ω–∞–ª—É) ---
if __name__ == "__main__":
    import uvicorn
    print("üöÄ –ó–∞–ø—É—Å–∫ ML API-—Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ http://localhost:8000")
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=True)